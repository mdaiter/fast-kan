{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5a5054-7017-4139-943d-d3e8b0999b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METAL\n"
     ]
    }
   ],
   "source": [
    "from tinygrad import Device\n",
    "print (Device.DEFAULT)\n",
    "from typing import *\n",
    "from tinygrad import TinyJit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "922745a9-09f7-41f2-a406-d31c59d45c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, nn\n",
    "import tinygrad.function as F\n",
    "import numpy as np\n",
    "\n",
    "class SplineLinearFunction:\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int, out_features: int, init_scale: float = 0.1\n",
    "    ):\n",
    "        self.init_scale = init_scale\n",
    "        self.linear_function = nn.Linear(in_features, out_features, bias=False)\n",
    "    \n",
    "    def __call__(self, x:Tensor) -> Tensor:\n",
    "        return self.linear_function(x)\n",
    "\n",
    "class RadialBasisFunction:\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_min = -2.,\n",
    "        grid_max = 2.,\n",
    "        num_grids = 8,\n",
    "        denominator = None\n",
    "    ):\n",
    "        self.grid_min = grid_min\n",
    "        self.grid_max = grid_max\n",
    "        self.num_grids = num_grids\n",
    "        # You don't need a special Parameter initialization here.\n",
    "        # You can initialize a Tensor later with this\n",
    "        self.grid = Tensor(np.linspace(grid_min, grid_max, num_grids, dtype=np.float32), requires_grad=True)\n",
    "        self.denominator = denominator or (grid_max - grid_min) / (num_grids - 1)\n",
    "\n",
    "    def __call__(self, x:Tensor) -> Tensor:\n",
    "        return (-(((x[..., None] - self.grid) / self.denominator).pow(2))).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4184fb72-c4a7-4ac6-b9e2-1648335cf57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tensor <LB METAL (4, 8) float (<UnaryOps.EXP2: 1>, None)> on METAL with grad None>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Tensor <LB METAL (3,) float ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))> on METAL with grad None>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf = RadialBasisFunction()\n",
    "print(rbf(Tensor([0., 1., 3., 10.])))\n",
    "\n",
    "slf = SplineLinearFunction(1, 3, 1)\n",
    "slf(Tensor([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3f6838a0-9247-455f-8146-2dc53fdb6131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastKANLayer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        grid_min: float = -2.,\n",
    "        grid_max: float = 2.,\n",
    "        num_grids: int = 8,\n",
    "        use_base_update: bool = True,\n",
    "        use_layernorm: bool = True,\n",
    "        base_activation = Tensor.silu,\n",
    "        spline_weight_init_scale: float = 0.1\n",
    "    ) -> None:\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        # normally you'd init layernorm here.\n",
    "        # but because layernorm *isn't* a layer in tinygrad,\n",
    "        # it's a function, I'm gonna hold off until the call\n",
    "        self.layernorm = None\n",
    "        if use_layernorm:\n",
    "            assert input_dim > 1, \"Do not use layernorms on 1D inputs. Set `use_layernorm=False`.\"\n",
    "            self.layernorm = nn.LayerNorm(input_dim)\n",
    "        self.rbf = RadialBasisFunction(grid_min, grid_max, num_grids)\n",
    "        self.spline_linear = SplineLinearFunction(input_dim * num_grids, output_dim, spline_weight_init_scale)\n",
    "        self.use_base_update = use_base_update\n",
    "        if use_base_update:\n",
    "            self.base_activation = base_activation\n",
    "            self.base_linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def __call__(\n",
    "        self, x: Tensor, use_layernorm=True\n",
    "    ) -> Tensor:\n",
    "        if self.layernorm is not None and use_layernorm:\n",
    "            spline_basis = self.rbf(self.layernorm(x))\n",
    "        else:\n",
    "            spline_basis = self.rbf(x)\n",
    "        spline_basis_view = spline_basis.view(*spline_basis.shape[:-2], -1)\n",
    "        ret = self.spline_linear(spline_basis_view)\n",
    "        if self.use_base_update:\n",
    "            base = self.base_linear(self.base_activation(x))\n",
    "            ret = ret + base\n",
    "        return ret\n",
    "\n",
    "    def plot_curve(\n",
    "        self,\n",
    "        input_index: int,\n",
    "        output_index: int,\n",
    "        num_pts: int = 1000,\n",
    "        num_extrapolate_bins: int = 2\n",
    "    ):\n",
    "        '''this function returns the learned curves in a FastKANLayer.\n",
    "        input_index: the selected index of the input, in [0, input_dim) .\n",
    "        output_index: the selected index of the output, in [0, output_dim) .\n",
    "        num_pts: num of points sampled for the curve.\n",
    "        num_extrapolate_bins (N_e): num of bins extrapolating from the given grids. The curve \n",
    "            will be calculate in the range of [grid_min - h * N_e, grid_max + h * N_e].\n",
    "        '''\n",
    "        ng = self.rbf.num_grids\n",
    "        h = self.rbf.denominator\n",
    "        assert input_index < self.input_dim\n",
    "        assert output_index < self.output_dim\n",
    "        w = self.spline_linear.linear_function.weight[\n",
    "            output_index, input_index * ng : (input_index + 1) * ng\n",
    "        ]   # num_grids,\n",
    "        x = Tensor(np.linspace(\n",
    "            self.rbf.grid_min - num_extrapolate_bins * h,\n",
    "            self.rbf.grid_max + num_extrapolate_bins * h,\n",
    "            num_pts\n",
    "        ))   # num_pts, num_grids\n",
    "        Tensor.no_grad = True\n",
    "        y = (w * self.rbf(x)).sum(-1)\n",
    "        Tensor.no_grad = False\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a28a1eea-71f7-4cc3-9602-da1ab48cdf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <LB METAL (2, 2) float (<BinaryOps.ADD: 1>, None)> on METAL with grad None>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastKANLayer = FastKANLayer(2, 2)\n",
    "fastKANLayer(Tensor([[1, 2], [1, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bc957af7-a460-48a8-ac57-482d67363048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:21: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\p'\n",
      "/var/folders/w1/gydggfx96qv449qgnv_xxj000000gn/T/ipykernel_61373/2906019297.py:21: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  plt.ylabel(\"$\\phi_{p,q}(x)$\")\n"
     ]
    },
    {
     "ename": "CompileError",
     "evalue": "Error Domain=MTLLibraryErrorDomain Code=3 \"program_source:3:28: error: 'double' is not supported in Metal\nkernel void E_2_4n2(device double* data0, const device float* data1, uint3 gid [[threadgroup_position_in_grid]], uint3 lid [[thread_position_in_threadgroup]]) {\n                           ^\nprogram_source:7:22: error: 'double' is not supported in Metal\n  *(data0+alu0+1) = (double)(((0.5f*val0.y)+(-0.25f)));\n                     ^\nprogram_source:8:22: error: 'double' is not supported in Metal\n  *(data0+alu0+2) = (double)(((0.5f*val0.z)+(-0.25f)));\n                     ^\nprogram_source:9:22: error: 'double' is not supported in Metal\n  *(data0+alu0+3) = (double)(((0.5f*val0.w)+(-0.25f)));\n                     ^\nprogram_source:10:20: error: 'double' is not supported in Metal\n  *(data0+alu0) = (double)(((0.5f*val0.x)+(-0.25f)));\n                   ^\n\" UserInfo={NSLocalizedDescription=program_source:3:28: error: 'double' is not supported in Metal\nkernel void E_2_4n2(device double* data0, const device float* data1, uint3 gid [[threadgroup_position_in_grid]], uint3 lid [[thread_position_in_threadgroup]]) {\n                           ^\nprogram_source:7:22: error: 'double' is not supported in Metal\n  *(data0+alu0+1) = (double)(((0.5f*val0.y)+(-0.25f)));\n                     ^\nprogram_source:8:22: error: 'double' is not supported in Metal\n  *(data0+alu0+2) = (double)(((0.5f*val0.z)+(-0.25f)));\n                     ^\nprogram_source:9:22: error: 'double' is not supported in Metal\n  *(data0+alu0+3) = (double)(((0.5f*val0.w)+(-0.25f)));\n                     ^\nprogram_source:10:20: error: 'double' is not supported in Metal\n  *(data0+alu0) = (double)(((0.5f*val0.x)+(-0.25f)));\n                   ^\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/runtime/ops_metal.py:25\u001b[0m, in \u001b[0;36mMetalCompiler.compile\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     24\u001b[0m options\u001b[38;5;241m.\u001b[39msetFastMathEnabled_(getenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMETAL_FAST_MATH\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: library \u001b[38;5;241m=\u001b[39m \u001b[43munwrap2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewLibraryWithSource_options_error_\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m CompileError(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/helpers.py:55\u001b[0m, in \u001b[0;36munwrap2\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     54\u001b[0m ret, err \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mAssertionError\u001b[0m: Error Domain=MTLLibraryErrorDomain Code=3 \"program_source:3:28: error: 'double' is not supported in Metal\nkernel void E_2_4n2(device double* data0, const device float* data1, uint3 gid [[threadgroup_position_in_grid]], uint3 lid [[thread_position_in_threadgroup]]) {\n                           ^\nprogram_source:7:22: error: 'double' is not supported in Metal\n  *(data0+alu0+1) = (double)(((0.5f*val0.y)+(-0.25f)));\n                     ^\nprogram_source:8:22: error: 'double' is not supported in Metal\n  *(data0+alu0+2) = (double)(((0.5f*val0.z)+(-0.25f)));\n                     ^\nprogram_source:9:22: error: 'double' is not supported in Metal\n  *(data0+alu0+3) = (double)(((0.5f*val0.w)+(-0.25f)));\n                     ^\nprogram_source:10:20: error: 'double' is not supported in Metal\n  *(data0+alu0) = (double)(((0.5f*val0.x)+(-0.25f)));\n                   ^\n\" UserInfo={NSLocalizedDescription=program_source:3:28: error: 'double' is not supported in Metal\nkernel void E_2_4n2(device double* data0, const device float* data1, uint3 gid [[threadgroup_position_in_grid]], uint3 lid [[thread_position_in_threadgroup]]) {\n                           ^\nprogram_source:7:22: error: 'double' is not supported in Metal\n  *(data0+alu0+1) = (double)(((0.5f*val0.y)+(-0.25f)));\n                     ^\nprogram_source:8:22: error: 'double' is not supported in Metal\n  *(data0+alu0+2) = (double)(((0.5f*val0.z)+(-0.25f)));\n                     ^\nprogram_source:9:22: error: 'double' is not supported in Metal\n  *(data0+alu0+3) = (double)(((0.5f*val0.w)+(-0.25f)));\n                     ^\nprogram_source:10:20: error: 'double' is not supported in Metal\n  *(data0+alu0) = (double)(((0.5f*val0.x)+(-0.25f)));\n                   ^\n}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mCompileError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_out):\n\u001b[1;32m     18\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mplot_curve(i, j, \u001b[38;5;241m200\u001b[39m, num_extrapolate_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m         plt\u001b[38;5;241m.\u001b[39mplot(x\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mphi_\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$x$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mphi_\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mp,q}(x)$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/tensor.py:3380\u001b[0m, in \u001b[0;36m_metadata_wrapper.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 3380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _METADATA\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3382\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m TRACEMETA \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   3383\u001b[0m     caller_frame \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe(frame \u001b[38;5;241m:=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/tensor.py:305\u001b[0m, in \u001b[0;36mTensor.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _to_np_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno np dtype for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m all_int(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno data if shape is symbolic, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mfrombuffer(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m_to_np_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/tensor.py:3380\u001b[0m, in \u001b[0;36m_metadata_wrapper.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 3380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _METADATA\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3382\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m TRACEMETA \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   3383\u001b[0m     caller_frame \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe(frame \u001b[38;5;241m:=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/tensor.py:249\u001b[0m, in \u001b[0;36mTensor._data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(\u001b[38;5;28mbytearray\u001b[39m(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# NOTE: this realizes on the object from as_buffer being a Python object\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m cpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCLANG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m buf \u001b[38;5;241m=\u001b[39m cast(Buffer, cast(LazyBuffer, cpu\u001b[38;5;241m.\u001b[39mlazydata)\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mrealized)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLANG\u001b[39m\u001b[38;5;124m\"\u001b[39m: buf\u001b[38;5;241m.\u001b[39moptions \u001b[38;5;241m=\u001b[39m BufferOptions(nolru\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/tensor.py:3380\u001b[0m, in \u001b[0;36m_metadata_wrapper.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 3380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _METADATA\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3382\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m TRACEMETA \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   3383\u001b[0m     caller_frame \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe(frame \u001b[38;5;241m:=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/tensor.py:208\u001b[0m, in \u001b[0;36mTensor.realize\u001b[0;34m(self, do_update_stats, *lst)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrealize\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mlst:Tensor, do_update_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Triggers the computation needed to create these Tensor(s).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m   \u001b[43mrun_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule_with_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlst\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_update_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_update_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/realize.py:221\u001b[0m, in \u001b[0;36mrun_schedule\u001b[0;34m(schedule, var_vals, do_update_stats)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_schedule\u001b[39m(schedule:List[ScheduleItem], var_vals:Optional[Dict[Variable, \u001b[38;5;28mint\u001b[39m]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, do_update_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 221\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mei\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlower_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschedule\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcapturing\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mCAPTURING\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapturing\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mei\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mei\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_update_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_update_stats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/realize.py:214\u001b[0m, in \u001b[0;36mlower_schedule\u001b[0;34m(schedule)\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor operations:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m   pprint\u001b[38;5;241m.\u001b[39mpprint(si\u001b[38;5;241m.\u001b[39mmetadata, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/realize.py:208\u001b[0m, in \u001b[0;36mlower_schedule\u001b[0;34m(schedule)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(schedule):\n\u001b[1;32m    207\u001b[0m   si \u001b[38;5;241m=\u001b[39m schedule\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 208\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mlower_schedule_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43msi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/realize.py:192\u001b[0m, in \u001b[0;36mlower_schedule_item\u001b[0;34m(si)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(x\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m si\u001b[38;5;241m.\u001b[39mbufs)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (si\u001b[38;5;241m.\u001b[39mast\u001b[38;5;241m.\u001b[39mop \u001b[38;5;129;01mis\u001b[39;00m UOps\u001b[38;5;241m.\u001b[39mEXT \u001b[38;5;129;01mand\u001b[39;00m si\u001b[38;5;241m.\u001b[39mast\u001b[38;5;241m.\u001b[39marg[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m MetaOps\u001b[38;5;241m.\u001b[39mCOPY)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m si\u001b[38;5;241m.\u001b[39mast\u001b[38;5;241m.\u001b[39mop \u001b[38;5;129;01mis\u001b[39;00m UOps\u001b[38;5;241m.\u001b[39mSINK:\n\u001b[0;32m--> 192\u001b[0m   runner \u001b[38;5;241m=\u001b[39m \u001b[43mget_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43msi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mast\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ExecItem(runner, [si\u001b[38;5;241m.\u001b[39mbufs[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mp\u001b[38;5;241m.\u001b[39mglobals], si\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m    194\u001b[0m out, (op, arg) \u001b[38;5;241m=\u001b[39m si\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m], si\u001b[38;5;241m.\u001b[39mast\u001b[38;5;241m.\u001b[39marg\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/realize.py:161\u001b[0m, in \u001b[0;36mget_runner\u001b[0;34m(dname, ast)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuzz_uops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UOpsFuzzerRunner\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UOpsFuzzerRunner(replace(prg, dname\u001b[38;5;241m=\u001b[39mdname))\n\u001b[0;32m--> 161\u001b[0m   method_cache[ckey] \u001b[38;5;241m=\u001b[39m method_cache[bkey] \u001b[38;5;241m=\u001b[39m ret \u001b[38;5;241m=\u001b[39m \u001b[43mCompiledRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/realize.py:83\u001b[0m, in \u001b[0;36mCompiledRunner.__init__\u001b[0;34m(self, p, precompiled)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m: \u001b[38;5;28mprint\u001b[39m(p\u001b[38;5;241m.\u001b[39msrc)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp:Program \u001b[38;5;241m=\u001b[39m p\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlib:\u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m precompiled \u001b[38;5;28;01mif\u001b[39;00m precompiled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDevice\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclprg \u001b[38;5;241m=\u001b[39m Device[p\u001b[38;5;241m.\u001b[39mdname]\u001b[38;5;241m.\u001b[39mruntime(p\u001b[38;5;241m.\u001b[39mfunction_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlib)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(p\u001b[38;5;241m.\u001b[39mname, p\u001b[38;5;241m.\u001b[39mdname, p\u001b[38;5;241m.\u001b[39mop_estimate, p\u001b[38;5;241m.\u001b[39mmem_estimate, p\u001b[38;5;241m.\u001b[39mlds_estimate)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/device.py:182\u001b[0m, in \u001b[0;36mCompiler.compile_cached\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcachekey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (lib \u001b[38;5;241m:=\u001b[39m diskcache_get(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcachekey, src)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m getenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASSERT_COMPILE\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried to compile with ASSERT_COMPILE set\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 182\u001b[0m   lib \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcachekey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: diskcache_put(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcachekey, src, lib)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lib\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/runtime/ops_metal.py:26\u001b[0m, in \u001b[0;36mMetalCompiler.compile\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     24\u001b[0m options\u001b[38;5;241m.\u001b[39msetFastMathEnabled_(getenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMETAL_FAST_MATH\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: library \u001b[38;5;241m=\u001b[39m unwrap2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mnewLibraryWithSource_options_error_(src, options, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28;01mraise\u001b[39;00m CompileError(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m library\u001b[38;5;241m.\u001b[39mlibraryDataContents()\u001b[38;5;241m.\u001b[39mbytes()\u001b[38;5;241m.\u001b[39mtobytes()\n",
      "\u001b[0;31mCompileError\u001b[0m: Error Domain=MTLLibraryErrorDomain Code=3 \"program_source:3:28: error: 'double' is not supported in Metal\nkernel void E_2_4n2(device double* data0, const device float* data1, uint3 gid [[threadgroup_position_in_grid]], uint3 lid [[thread_position_in_threadgroup]]) {\n                           ^\nprogram_source:7:22: error: 'double' is not supported in Metal\n  *(data0+alu0+1) = (double)(((0.5f*val0.y)+(-0.25f)));\n                     ^\nprogram_source:8:22: error: 'double' is not supported in Metal\n  *(data0+alu0+2) = (double)(((0.5f*val0.z)+(-0.25f)));\n                     ^\nprogram_source:9:22: error: 'double' is not supported in Metal\n  *(data0+alu0+3) = (double)(((0.5f*val0.w)+(-0.25f)));\n                     ^\nprogram_source:10:20: error: 'double' is not supported in Metal\n  *(data0+alu0) = (double)(((0.5f*val0.x)+(-0.25f)));\n                   ^\n\" UserInfo={NSLocalizedDescription=program_source:3:28: error: 'double' is not supported in Metal\nkernel void E_2_4n2(device double* data0, const device float* data1, uint3 gid [[threadgroup_position_in_grid]], uint3 lid [[thread_position_in_threadgroup]]) {\n                           ^\nprogram_source:7:22: error: 'double' is not supported in Metal\n  *(data0+alu0+1) = (double)(((0.5f*val0.y)+(-0.25f)));\n                     ^\nprogram_source:8:22: error: 'double' is not supported in Metal\n  *(data0+alu0+2) = (double)(((0.5f*val0.z)+(-0.25f)));\n                     ^\nprogram_source:9:22: error: 'double' is not supported in Metal\n  *(data0+alu0+3) = (double)(((0.5f*val0.w)+(-0.25f)));\n                     ^\nprogram_source:10:20: error: 'double' is not supported in Metal\n  *(data0+alu0) = (double)(((0.5f*val0.x)+(-0.25f)));\n                   ^\n}"
     ]
    }
   ],
   "source": [
    "from tinygrad import dtypes\n",
    "d_in = 2\n",
    "d_out = 3\n",
    "\n",
    "layer = FastKANLayer(\n",
    "    d_in, d_out,\n",
    "    use_base_update=False,\n",
    "    use_layernorm=False\n",
    ")\n",
    "\n",
    "x, y = layer.plot_curve(0, 1, num_pts=1000, num_extrapolate_bins=3)\n",
    "x.shape, y.shape\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(d_in):\n",
    "    for j in range(d_out):\n",
    "        x, y = layer.plot_curve(i, j, 200, num_extrapolate_bins=3)\n",
    "        plt.plot(x.numpy(), y.numpy(), label=r\"$\\phi_{\" + f\"{i},{j}\" + r\"}$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$\\phi_{p,q}(x)$\")\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b98d7920-0f18-4813-892d-9bb432095ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastKAN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers_hidden: List[int],\n",
    "        grid_min: float = -2.,\n",
    "        grid_max: float = 2.,\n",
    "        num_grids: int = 8,\n",
    "        use_base_update: bool = True,\n",
    "        base_activation = Tensor.silu,\n",
    "        spline_weight_init_scale: float = 0.1,\n",
    "    ) -> None:\n",
    "        self.layers = [\n",
    "            FastKANLayer(\n",
    "                in_dim, out_dim,\n",
    "                grid_min=grid_min,\n",
    "                grid_max=grid_max,\n",
    "                num_grids=num_grids,\n",
    "                use_base_update=use_base_update,\n",
    "                base_activation=base_activation,\n",
    "                spline_weight_init_scale=spline_weight_init_scale,\n",
    "            ) for in_dim, out_dim in zip(layers_hidden[:-1], layers_hidden[1:])\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x:Tensor) -> Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a277cad6-2777-444b-a26a-477ed256fd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <LB METAL (3, 1) float (<BinaryOps.ADD: 1>, None)> on METAL with grad None>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastKAN = FastKAN([2, 3, 1])\n",
    "fastKAN(Tensor([[2, 1], [1, 2], [5, 9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5c6a297c-da13-481c-ad07-0c8428c1896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithFastKANTransform:\n",
    "    def __init__(\n",
    "        self,\n",
    "        q_dim: int,\n",
    "        k_dim: int,\n",
    "        v_dim: int,\n",
    "        head_dim: int,\n",
    "        num_heads: int,\n",
    "        gating: bool = True,\n",
    "    ):\n",
    "        self.num_heads = num_heads\n",
    "        total_dim = head_dim * self.num_heads\n",
    "        self.gating = gating\n",
    "        self.linear_q = FastKANLayer(q_dim, total_dim)\n",
    "        self.linear_k = FastKANLayer(k_dim, total_dim)\n",
    "        self.linear_v = FastKANLayer(v_dim, total_dim)\n",
    "        self.linear_o = FastKANLayer(total_dim, q_dim)\n",
    "        self.linear_g = None\n",
    "        if self.gating:\n",
    "            self.linear_g = FastKANLayer(q_dim, total_dim)\n",
    "        # precompute the 1/sqrt(head_dim)\n",
    "        self.norm = head_dim**-0.5\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        q: Tensor,\n",
    "        k: Tensor,\n",
    "        v: Tensor,\n",
    "        bias: Tensor = None,      # additive attention bias\n",
    "    ) -> Tensor:\n",
    "\n",
    "        wq = self.linear_q(q).view(*q.shape[:-1], 1, self.num_heads, -1) * self.norm     # *q1hc\n",
    "        wk = self.linear_k(k).view(*k.shape[:-2], 1, k.shape[-2], self.num_heads, -1)    # *1khc\n",
    "        att = (wq * wk).sum(-1).softmax(-2)     # *qkh\n",
    "        del wq, wk\n",
    "        if bias is not None:\n",
    "            att = att + bias[..., None]\n",
    "\n",
    "        wv = self.linear_v(v).view(*v.shape[:-2],1, v.shape[-2], self.num_heads, -1)     # *1khc\n",
    "        o = (att[..., None] * wv).sum(-3)        # *qhc\n",
    "        del att, wv\n",
    "\n",
    "        o = o.view(*o.shape[:-2], -1)           # *q(hc)\n",
    "\n",
    "        if self.linear_g is not None:\n",
    "            # gating, use raw query input\n",
    "            g = self.linear_g(q)\n",
    "            o = Tensor.sigmoid(g) * o\n",
    "\n",
    "        # merge heads\n",
    "        o = self.linear_o(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a6072486-7be0-4597-b384-49b422c8a6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test attention: attention with fast kan transform got correct shapes.\n"
     ]
    }
   ],
   "source": [
    "batch_shape = (1,)\n",
    "num_q = 12\n",
    "num_kv = 24\n",
    "q_dim = k_dim = v_dim = 32\n",
    "head_dim = 8\n",
    "num_heads = 2\n",
    "q = Tensor.randn(*batch_shape, num_q, q_dim)\n",
    "k = Tensor.randn(*batch_shape, num_kv, k_dim)\n",
    "v = Tensor.randn(*batch_shape, num_kv, v_dim)\n",
    "\n",
    "fast_kan_att = AttentionWithFastKANTransform(q_dim, k_dim, v_dim, head_dim, num_heads, gating=True)\n",
    "out = fast_kan_att(q, k, v, bias=None)\n",
    "assert out.shape == q.shape, out.shape\n",
    "\n",
    "bias = Tensor.rand(*batch_shape, num_q, num_kv)\n",
    "out = fast_kan_att(q, k, v, bias=bias)\n",
    "assert out.shape == q.shape, out.shape\n",
    "\n",
    "print(\"test attention: attention with fast kan transform got correct shapes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f5fb137-bcff-4982-83b0-c95579a0b5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28) dtypes.uchar (60000,) dtypes.uchar\n",
      "(10000, 1, 28, 28) dtypes.uchar (10000,) dtypes.uchar\n"
     ]
    }
   ],
   "source": [
    "#Okay, we've ascended the hill!\n",
    "# Time to train MNIST\n",
    "\n",
    "from tinygrad.nn.datasets import mnist\n",
    "X_train, Y_train, X_test, Y_test = mnist()\n",
    "print(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n",
    "print(X_test.shape, X_test.dtype, Y_test.shape, Y_test.dtype)\n",
    "# (60000, 1, 28, 28) dtypes.uchar (60000,) dtypes.uchar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f50ecf8-edaa-4a29-bc77-96a115dcf86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastKAN([28 * 28, 64, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8c8dbe2-ad60-491f-930b-4e29b72a3e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tensor <LB METAL (10000,) int (<BinaryOps.ADD: 1>, None)> on METAL with grad None>\n",
      "(10000,)\n",
      "0.08649999648332596\n"
     ]
    }
   ],
   "source": [
    "print(model(X_test.view(-1, 28 * 28)).argmax(axis=1))\n",
    "print(Y_test.shape)\n",
    "\n",
    "acc = (model(X_test.view(-1, 28 * 28)).argmax(axis=1) == Y_test).mean()\n",
    "# NOTE: tinygrad is lazy, and hasn't actually run anything by this point\n",
    "print(acc.item())  # ~10% accuracy, as expected from a random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2c4d91c-7e08-4db2-a844-ed9de2f43304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Tensor <LB METAL (784,) float ShapeTracker(views=(View(shape=(784,), strides=(0,), offset=0, mask=None, contiguous=False),))> on METAL with grad None>, <Tensor <LB METAL (784,) float ShapeTracker(views=(View(shape=(784,), strides=(0,), offset=0, mask=None, contiguous=False),))> on METAL with grad None>, <Tensor <LB METAL (8,) float (<MetaOps.COPY: 3>, <buf real:True device:METAL size:8 dtype:dtypes.float offset:0>)> on METAL with grad <LB METAL (8,) float (<BinaryOps.ADD: 1>, None)>>, <Tensor <LB METAL (64, 6272) float (<BinaryOps.ADD: 1>, <buf real:True device:METAL size:401408 dtype:dtypes.float offset:0>)> on METAL with grad None>, <Tensor <LB METAL (64, 784) float (<BinaryOps.ADD: 1>, <buf real:True device:METAL size:50176 dtype:dtypes.float offset:0>)> on METAL with grad None>, <Tensor <LB METAL (64,) float (<BinaryOps.ADD: 1>, <buf real:True device:METAL size:64 dtype:dtypes.float offset:0>)> on METAL with grad None>, <Tensor <LB METAL (64,) float ShapeTracker(views=(View(shape=(64,), strides=(0,), offset=0, mask=None, contiguous=False),))> on METAL with grad None>, <Tensor <LB METAL (64,) float ShapeTracker(views=(View(shape=(64,), strides=(0,), offset=0, mask=None, contiguous=False),))> on METAL with grad None>, <Tensor <LB METAL (8,) float (<MetaOps.COPY: 3>, <buf real:True device:METAL size:8 dtype:dtypes.float offset:0>)> on METAL with grad <LB METAL (8,) float (<BinaryOps.ADD: 1>, None)>>, <Tensor <LB METAL (10, 512) float (<BinaryOps.ADD: 1>, <buf real:True device:METAL size:5120 dtype:dtypes.float offset:0>)> on METAL with grad None>, <Tensor <LB METAL (10, 64) float (<BinaryOps.ADD: 1>, <buf real:True device:METAL size:640 dtype:dtypes.float offset:0>)> on METAL with grad None>, <Tensor <LB METAL (10,) float (<BinaryOps.ADD: 1>, <buf real:True device:METAL size:10 dtype:dtypes.float offset:0>)> on METAL with grad None>]\n"
     ]
    }
   ],
   "source": [
    "print(nn.state.get_parameters(model))\n",
    "optim = nn.optim.AdamW(nn.state.get_parameters(model), lr=1e-3, weight_decay=1e-4)\n",
    "batch_size = 128\n",
    "def step():\n",
    "  Tensor.training = True  # makes dropout work\n",
    "  samples = Tensor.randint(batch_size, high=X_train.shape[0])\n",
    "  X, Y = X_train[samples], Y_train[samples]\n",
    "  optim.zero_grad()\n",
    "  loss = (model(X.view(-1, 28 * 28)) + 1e-8).sparse_categorical_crossentropy(Y).backward()\n",
    "  optim.step()\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a830e30-57f1-4ccf-bac5-a35592cae10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6120352919679135,\n",
       " 0.22639587498269975,\n",
       " 0.026706916047260165,\n",
       " 0.03579529095441103,\n",
       " 0.02406612504273653]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "timeit.repeat(step, repeat=5, number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aef1c655-2d4c-41ac-9a03-67fac5275b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import TinyJit\n",
    "jit_step = TinyJit(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2f917eb-5046-4287-85d4-7a4ab8d70d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08452891698107123,\n",
       " 0.028836792102083564,\n",
       " 0.025331582874059677,\n",
       " 0.026197542203590274,\n",
       " 0.02121345791965723]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "timeit.repeat(jit_step, repeat=5, number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd0245b2-de81-414a-a7cf-8b4620f590d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0, loss 0.08, acc 96.13%\n",
      "step  100, loss 0.05, acc 96.18%\n",
      "step  200, loss 0.08, acc 96.10%\n",
      "step  300, loss 0.05, acc 96.36%\n",
      "step  400, loss 0.04, acc 96.64%\n",
      "step  500, loss 0.10, acc 96.41%\n",
      "step  600, loss 0.11, acc 96.29%\n",
      "step  700, loss 0.10, acc 96.42%\n",
      "step  800, loss 0.03, acc 96.57%\n",
      "step  900, loss 0.04, acc 96.51%\n",
      "step 1000, loss 0.03, acc 96.54%\n",
      "step 1100, loss 0.09, acc 96.51%\n",
      "step 1200, loss 0.02, acc 96.45%\n",
      "step 1300, loss 0.04, acc 96.68%\n",
      "step 1400, loss 0.11, acc 96.94%\n",
      "step 1500, loss 0.06, acc 96.70%\n",
      "step 1600, loss 0.03, acc 96.51%\n",
      "step 1700, loss 0.08, acc 96.71%\n",
      "step 1800, loss 0.02, acc 96.37%\n",
      "step 1900, loss 0.04, acc 96.81%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Context(BEAM\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7000\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mjit_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m       Tensor\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/jit.py:273\u001b[0m, in \u001b[0;36mTinyJit.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptured\u001b[38;5;241m.\u001b[39mexpected_names \u001b[38;5;241m==\u001b[39m names, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs mismatch in JIT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptured\u001b[38;5;241m.\u001b[39mexpected_names\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnames\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptured\u001b[38;5;241m.\u001b[39mexpected_st_vars_dtype_device \u001b[38;5;241m==\u001b[39m st_vars_dtype_device, \\\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs mismatch in JIT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptured\u001b[38;5;241m.\u001b[39mexpected_st_vars_dtype_device\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mst_vars_dtype_device\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 273\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_buffers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_vals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/jit.py:167\u001b[0m, in \u001b[0;36mCapturedJit.__call__\u001b[0;34m(self, input_buffers, var_vals)\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graphed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjit execs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_cache)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m kernels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ei \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_cache: \u001b[43mei\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_inputs()\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mret\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/realize.py:173\u001b[0m, in \u001b[0;36mExecItem.run\u001b[0;34m(self, var_vals, wait, jit, do_update_stats)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, var_vals:Optional[Dict[Variable, \u001b[38;5;28mint\u001b[39m]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, jit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, do_update_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    172\u001b[0m   bufs \u001b[38;5;241m=\u001b[39m [cast(Buffer, x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbufs] \u001b[38;5;28;01mif\u001b[39;00m jit \u001b[38;5;28;01melse\u001b[39;00m [cast(Buffer, x)\u001b[38;5;241m.\u001b[39mensure_allocated() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbufs]\n\u001b[0;32m--> 173\u001b[0m   et \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_vals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvar_vals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDEBUG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m do_update_stats:\n\u001b[1;32m    175\u001b[0m     GlobalCounters\u001b[38;5;241m.\u001b[39mkernel_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/realize.py:110\u001b[0m, in \u001b[0;36mCustomOp.__call__\u001b[0;34m(self, rawbufs, var_vals, wait)\u001b[0m\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, rawbufs:List[Buffer], var_vals:Dict[Variable, \u001b[38;5;28mint\u001b[39m], wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfxn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrawbufs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/tensor.py:3376\u001b[0m, in \u001b[0;36mcustom_random\u001b[0;34m(out)\u001b[0m\n\u001b[1;32m   3374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mhalf: rng_np_buffer \u001b[38;5;241m=\u001b[39m (rng\u001b[38;5;241m.\u001b[39mintegers(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2047\u001b[39m, size\u001b[38;5;241m=\u001b[39mout\u001b[38;5;241m.\u001b[39msize) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2048\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mhalf, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: rng_np_buffer \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mrandom(size\u001b[38;5;241m=\u001b[39mout\u001b[38;5;241m.\u001b[39msize, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39m_to_np_dtype(out\u001b[38;5;241m.\u001b[39mdtype), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 3376\u001b[0m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng_np_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/device.py:118\u001b[0m, in \u001b[0;36mBuffer.copyin\u001b[0;34m(self, mv)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mv) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbytes, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize mismatch, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(mv)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_allocated(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt copyin to unallocated buffer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallocator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/runtime/ops_metal.py:98\u001b[0m, in \u001b[0;36mMetalAllocator.copyin\u001b[0;34m(self, dest, src)\u001b[0m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopyin\u001b[39m(\u001b[38;5;28mself\u001b[39m, dest:MetalBuffer, src:\u001b[38;5;28mmemoryview\u001b[39m): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdest\u001b[49m\u001b[43m)\u001b[49m[:] \u001b[38;5;241m=\u001b[39m src\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/runtime/ops_metal.py:96\u001b[0m, in \u001b[0;36mMetalAllocator.as_buffer\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mas_buffer\u001b[39m(\u001b[38;5;28mself\u001b[39m, src:MetalBuffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m:\n\u001b[0;32m---> 96\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mbuf\u001b[38;5;241m.\u001b[39mcontents()\u001b[38;5;241m.\u001b[39mas_buffer(src\u001b[38;5;241m.\u001b[39moffset\u001b[38;5;241m+\u001b[39msrc\u001b[38;5;241m.\u001b[39msize)[src\u001b[38;5;241m.\u001b[39moffset:]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/runtime/ops_metal.py:118\u001b[0m, in \u001b[0;36mMetalDevice.synchronize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msynchronize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 118\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m cbuf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmtl_buffers_in_flight: \u001b[43mwait_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmv_in_metal\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    120\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmtl_buffers_in_flight\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/runtime/ops_metal.py:10\u001b[0m, in \u001b[0;36mwait_check\u001b[0;34m(cbuf)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_check\u001b[39m(cbuf: Any):\n\u001b[0;32m---> 10\u001b[0m   \u001b[43mcbuf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitUntilCompleted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (error \u001b[38;5;241m:=\u001b[39m cbuf\u001b[38;5;241m.\u001b[39merror()) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tinygrad import Context\n",
    "with Context(BEAM=2):\n",
    "  for step in range(7000):\n",
    "    loss = jit_step()\n",
    "    if step%100 == 0:\n",
    "      Tensor.training = False\n",
    "      acc = (model(X_test.view(-1, 28 * 28)).argmax(axis=1) == Y_test).mean().item()\n",
    "      print(f\"step {step:4d}, loss {loss.item():.2f}, acc {acc*100.:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a9507c44-1d52-4522-a487-07dc232c4a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345 ns  157 ns per loop (mean  std. dev. of 10 runs, 1,000 loops each)\n",
      "304 ns  24.4 ns per loop (mean  std. dev. of 10 runs, 1,000 loops each)\n",
      "334 ns  43 ns per loop (mean  std. dev. of 10 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from tinygrad import TinyJit\n",
    "with Context(BEAM=2):\n",
    "    fklayer = FastKANLayer(100, 100)\n",
    "    x = Tensor.randn(8, 100)\n",
    "    rbf = RadialBasisFunction()\n",
    "    def fklayer_step():\n",
    "        fklayer(x, use_layernorm=False)\n",
    "    def fklayer_sum_backwards_test():\n",
    "        fklayer(x).sum().backward()\n",
    "    def rbf_test():\n",
    "        rbf(x)\n",
    "    %timeit -r10 -n1000 TinyJit(fklayer_step)\n",
    "    %timeit -r10 -n1000 TinyJit(fklayer_sum_backwards_test)\n",
    "    %timeit -r10 -n1000 TinyJit(rbf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a23f29c-da62-4e1a-a75e-b9501e6ef985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
